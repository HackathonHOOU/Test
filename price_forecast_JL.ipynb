# -*- coding: utf-8 -*-
"""
Created on Thu Feb 20 15:41:21 2020

@author: jelto
"""
import pandas as pd
import numpy as np
import os

from keras.models import Model
from keras.layers import Input, Dense, Dropout
from keras.layers import LSTM

from keras.regularizers import L1L2
from keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.preprocessing import MinMaxScaler

from matplotlib import pyplot as plt

power_prices_DA=pd.read_excel('PricesDayAhead2020.xlsx')
power_prices_DA = power_prices_DA['Day-ahead Price [EUR/MWh]'].loc[0:8785]
power_prices_DA = power_prices_DA.interpolate('linear')

if not os.path.exists('./power_prices/'):
    os.makedirs('./power_prices/'+ '/best_weights/')
current_folder = './power_prices/'

scaler_X = MinMaxScaler()
X = power_prices_DA.values.astype(np.float32).reshape(-1,1)
X_scaled = scaler_X.fit_transform(X)

scaler_Y = MinMaxScaler()
Y = power_prices_DA.shift(-24).values.astype(np.float32).reshape(-1,1)
Y_scaled = scaler_Y.fit_transform(Y)
#zwei drüber -1?
# Quick-and-dirty: Der erster Eintrag von Y ist "nan", daher muss er bei beiden 
# gelöscht werden. Richtiger wäre ein Suchen nach "nan" und ein indexbasiertes
# Löschen #shift und scaled denselben Wert, damit sauber abgeschnitten
X_scaled = X_scaled[24:]
Y_scaled = Y_scaled[24:]

t_steps_back = 24

X_LSTM, Y_LSTM = list(), list()

for i in range(t_steps_back,len(X_scaled)):
    # find the end of this pattern
    if i > len(X_scaled):
        break
    # gather input and output parts of the pattern
    seq_x, seq_y = X_scaled[(i-t_steps_back):i, :], Y_scaled[i]
                            
    X_LSTM.append(seq_x)
    Y_LSTM.append(seq_y)

X_LSTM = np.array(X_LSTM)
Y_LSTM = np.array(Y_LSTM)

test_steps = 720
train_inds, test_inds = np.array(range(X_LSTM.shape[0]))[:-test_steps], \
    np.array(range(X_LSTM.shape[0]))[-test_steps:]

X_train, X_test, Y_train, Y_test = X_LSTM[train_inds], X_LSTM[test_inds], \
    Y_LSTM[train_inds], Y_LSTM[test_inds]

# Sorry, hier gab es eine falsche Auskunft durch mich:
# Es muss (Anzahl Samples, Time steps, Anzahl features) sein. 
# Die von die geschriebenen reshape-Zeilen müssen also weg (auskommentiert)...
# X_test=X_test.reshape(-1,1,24)
# X_train=X_train.reshape(-1,1,24)

val_steps = 720
train_inds, val_inds = np.array(range(X_train.shape[0]))[:-val_steps], \
    np.array(range(X_train.shape[0]))[-val_steps:]
X_train, X_val, Y_train, Y_val = \
    X_train[train_inds], X_train[val_inds], \
    Y_train[train_inds], Y_train[val_inds]

earlystop = EarlyStopping(monitor='val_loss', min_delta=0., patience = 25, verbose = 0)
checkpoint = ModelCheckpoint((current_folder + 'best_weights/' + 'bw_power_prices.h5'),\
                             monitor='val_loss', verbose=False, save_weights_only=True, save_best_only=True)

input_shape= X_train.shape[1:]

# =============================================================================
# Modellaufbau
# =============================================================================
visible = Input(shape=input_shape)
lstm = LSTM(100)(visible)
fc1=Dense(100,'relu')(lstm)
fc2=Dense(100,'relu')(fc1)
fc3=Dense(100,'relu')(fc2)
fc4=Dense(100,'relu')(fc3)
fc5=Dense(100,'relu')(fc4)
#lstm1 = LSTM(200,kernel_regularizer=L1L2(l1=0.01, l2=0.0),return_sequences=True)(visible)
#lstm2 = LSTM(100,kernel_regularizer=L1L2(l1=0.01, l2=0.0))(lstm1)
#drpt = Dropout(rate=0.2)(lstm2)
# Nur als Hinweis, wie man die activation function in python definiert. 
# Das geht auch bei den LSTM-layern (allerdings nicht, bei den )
output = Dense(1)(fc5)#),activation='tanh')(drpt) 

model = Model(inputs=visible, outputs=output)
model.compile(loss='mae', optimizer='adam')
# =============================================================================
# Modellaufbau abgeschlossen
# =============================================================================

#try: 
#    num = int("Absichtlicher Fehler")
#except ValueError:
#    print("Die nächste Zeile sollte nun funktionieren, allerdings wird das " + 
#          "Training 'nan' ausgeben, weil in den Trainingsdaten noch 'nan'-" +
#          "Werte versteckt sind. Die müsstest du entweder in der Excel oder " +
#          "im Code nochmal filtern...\nIch habe außerdem das verbose-" +
#          "Statement auf 1 gesetzt, damit man den Prozess detaillierter " +
#          "verfolgen kann (wie sehen kannst).")

history = model.fit(X_train,Y_train,validation_data=(X_val,Y_val),\
                    epochs=100,verbose=1,callbacks=[earlystop,checkpoint])#,batch_size=25)
model.load_weights(current_folder + 'best_weights/' + 'bw_power_prices.h5')

Y_pred_train = model.predict(X_train)
Y_pred_val = model.predict(X_val)
Y_pred_test = model.predict(X_test)
#plt.plot(Y_pred_train)
plt.plot(Y_pred_val,'green')
plt.plot(Y_val,'red')
DiffY=Y_val-Y_pred_val
plt.plot(DiffY,'black')
#plt.plot(Y_pred_test)
